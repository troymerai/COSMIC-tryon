# -*- coding: utf-8 -*-
"""Cosmic-tryon-api-test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WykLbkR116ZJ6f7Cs0DuTCkr3uXwfYCF

# 1. 준비

Runtime Type = Python 3
Hardware Accelerator = GPU

그 다음에 환경 세팅

#모델에 필요한 파일 받기
"""

#현재 위치 확인
!pwd

#ACGPN 모델 파일 다운로드
!git clone https://github.com/kairess/ACGPN.git

# Commented out IPython magic to ensure Python compatibility.
#다운로드 받은 파일의 위치로 이동
# %cd ACGPN

"""#dependencies 및 사전 파일 준비"""

#gdown은 코랩에서만 되는듯? 그냥 리눅스에서는 wget 쓰는 걸로 알고있음
!pip install -U --no-cache-dir gdown --pre -qq
#얘는 필요 c++ 빌드 관련 모시기라고 함
!pip install ninja -qq

import gdown
import numpy as np
from PIL import Image
import IPython
import os
import sys
import time

from predict_pose import generate_pose_keypoints

#모델을 돌리기 위한 폴더 준비
!mkdir Data_preprocessing/test_color
!mkdir Data_preprocessing/test_colormask
!mkdir Data_preprocessing/test_edge
!mkdir Data_preprocessing/test_img
!mkdir Data_preprocessing/test_label
!mkdir Data_preprocessing/test_mask
!mkdir Data_preprocessing/test_pose
!mkdir inputs
!mkdir inputs/img
!mkdir inputs/cloth

#현재 위치 확인
!pwd

#휴먼 세그멘테이션 모델 파일 다운로드
!git clone https://github.com/levindabhi/Self-Correction-Human-Parsing-for-ACGPN.git
#옷 마스킹 모델 파일 다운로드
!git clone https://github.com/levindabhi/U-2-Net.git

"""#사전학습 모델 다운로드

## 포즈 예측 모델
"""

!gdown --id 11dRA_iLSJFFsOWXuadmH7Apqukc2F_Vf -O pose/pose_iter_440000.caffemodel

"""## 휴먼 세그멘테이션 모델"""

gdown.download('https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH', 'lip_final.pth', quiet=False)

"""## U-2-Net모델 (옷 마스크 추출 모델)"""

# Commented out IPython magic to ensure Python compatibility.
#U-2-Net 모델을 돌릴 때 필요한 파일 준비를 위해 해당 모델 폴더로 이동
# %cd U-2-Net

#U-2-Net 모델에 필요한 폴더 생성
!mkdir saved_models
!mkdir saved_models/u2net
!mkdir saved_models/u2netp

#사전 학습된 U-2-Net 모델 다운로드
!gdown 1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy -O saved_models/u2netp/u2netp.pth
!gdown 1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ -O saved_models/u2net/u2net.pth

import u2net_load
import u2net_run

#로드하려면 쿠다 필요 (위치 확인용)
u2net = u2net_load.model(model_name='u2netp')

#U-2-Net 모델을 위한 파일 준비가 끝나면 부모 폴더로 이동
# %cd ..

"""## ACGPN모델 (사진 합성 모델)"""

#ACGPN 모델에 필요한 폴더 생성
!mkdir checkpoints

#사전 학습된 ACGPN 모델 다운로드
gdown.download('https://drive.google.com/uc?id=1UWT6esQIU_d4tUm8cjxDKMhB8joQbrFx', output='checkpoints/ACGPN_checkpoints.zip', quiet=False)

#압축된 모델 파일 풀어서 전에 만든 폴더위치에 넣어줌
!unzip checkpoints/ACGPN_checkpoints.zip -d checkpoints

# Commented out IPython magic to ensure Python compatibility.
#위치 확인용
!pwd
# 부모 폴더로 이동
# %cd ..

"""# ESRGAN에 필요한 파일 받기"""

# Commented out IPython magic to ensure Python compatibility.
# Clone Real-ESRGAN and enter the Real-ESRGAN
!git clone https://github.com/xinntao/Real-ESRGAN.git
# %cd Real-ESRGAN
# Set up the environment
!pip install basicsr
!pip install facexlib
!pip install gfpgan
!pip install -r requirements.txt
!python setup.py develop

#현재 위치 확인
!pwd

"""# 업스케일링 폴더 생성"""

import os
#from google.colab import files
import shutil

upload_folder = 'upload'
result_folder = 'results'

if os.path.isdir(upload_folder):
    shutil.rmtree(upload_folder)
if os.path.isdir(result_folder):
    shutil.rmtree(result_folder)
os.mkdir(upload_folder)
os.mkdir(result_folder)

"""# ACGPN 돌리고 -> 업스케일링 폴더로 이동시키기 -> 업스케일링 진행 후 이미지 반환!

"""

# Commented out IPython magic to ensure Python compatibility.
#ACGPN 돌리기 위해서 ACGPN 디렉토리로 이동
# %cd ..
# %cd ACGPN

#옷 데이터셋이 올바른 위치에 있는지 확인
sorted(os.listdir('inputs/cloth'))

#전처리 할 옷 데이터 이름을 시간으로 지정
cloth_name = f'cloth_{int(time.time())}.png'

#전처리 할 옷 데이터의 파일 위치 확인
cloth_path = os.path.join('inputs/cloth', sorted(os.listdir('inputs/cloth'))[0])

#이미지 할당
cloth = Image.open(cloth_path)

#이미지 사이즈 조절 및 RGB모드로 변환
cloth = cloth.resize((192, 256), Image.BICUBIC).convert('RGB')

#전처리를 위해 옷 이미지 파일을 해당 파일 위치에 지정해준 옷 데이터 이름으로 저장
cloth.save(os.path.join('Data_preprocessing/test_color', cloth_name))

#전처리를 위해 U-2-Net모델을 지정한 파일 위치에서 데이터셋을 가저와서 돌리고, 지정한 위치에 전처리 된 옷 이미지 파일을 저장
u2net_run.infer(u2net, 'Data_preprocessing/test_color', 'Data_preprocessing/test_edge')

#결과 확인을 위해 전처리 된 옷 이미지 파일을 열어봄
Image.open(f'Data_preprocessing/test_edge/{cloth_name}')

#모델 데이터셋이 올바른 위치에 있는지 확인
sorted(os.listdir('inputs/img'))

#전처리 할 모델 데이터 이름을 시간으로 지정
img_name = f'img_{int(time.time())}.png'

#전처리 할 모델 데이터의 파일 위치 확인
img_path = os.path.join('inputs/img', sorted(os.listdir('inputs/img'))[0])

#이미지 할당
img = Image.open(img_path)

#이미지 사이즈 조절
img = img.resize((192,256), Image.BICUBIC)

#전처리를 위해 모델 이미지 파일의 위치값을 해당 파일 위치에 지정해준 모델 데이터 이름으로 지정
img_path = os.path.join('Data_preprocessing/test_img', img_name)

#지정해준 모델 이미지 파일의 위치값을 바탕으로 이미지 저장
img.save(img_path)

#전처리를 위해 휴먼 세그멘테이션 모델을 parser를 이용하여 돌림 ***parser는 데이터셋의 종류, 사전학습 모델 파일의 위치, 전처리 할 모델 이미지 위치, 전처리 후 이미지 저장할 위치
!python3 Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py --dataset 'lip' --model-restore 'lip_final.pth' --input-dir 'Data_preprocessing/test_img' --output-dir 'Data_preprocessing/test_label'

#모델 포즈 예측 모델을 돌리고 저장할 파일 위치와 파일 이름 형식을 지정
pose_path = os.path.join('Data_preprocessing/test_pose', img_name.replace('.png', '_keypoints.json'))

#모델 포즈 예측 모델을 전처리 할 모델 이미지 위치, 전처리 후 파일 위치를 받아서 돌리고 저장
generate_pose_keypoints(img_path, pose_path)

#기존에 존재하던 test_pairs.txt파일을 삭제 후 이번에 돌릴 이미지 이름들로 고쳐서 생성 *** 기존 test_pairs.txt파일에는 input으로 넣어준 옷 이미지 모델 이름과, 모델 이미지 이름이 존재
!rm -rf Data_preprocessing/test_pairs.txt
with open('Data_preprocessing/test_pairs.txt', 'w') as f:
    f.write(f'{img_name} {cloth_name}')

#ACGPN 추론 실행
!python test.py

#api에는 쓸모 없지만 테스트에는 필요해서 넣은 이미지 확인 코드
output_grid = np.concatenate([
    np.array(Image.open(f'Data_preprocessing/test_img/{img_name}')),
    np.array(Image.open(f'Data_preprocessing/test_color/{cloth_name}')),
    np.array(Image.open(f'results/test/try-on/{img_name}'))
], axis=1)

image_grid = Image.fromarray(output_grid)

image_grid

# Commented out IPython magic to ensure Python compatibility.
# 루트 디렉토리로 다시 감 (ACGPN 디렉토리 하의 폴더에서 Real-ESRGAN 하의 폴더로 파일을 보내야하기 때문에 루트 디렉토리에 있어야 함 )
!pwd
# %cd ..

from PIL import Image
from pathlib import Path
import os

CURRENT_DIR = Path()
q = CURRENT_DIR / 'Real-ESRGAN' / 'upload'
d = CURRENT_DIR / 'ACGPN' / 'results' / 'test' / 'try-on' / f"{img_name}"


file_path = f"/ACGPN/results/test/try-on/{img_name}"

# 이미지 파일을 변수에 로드
uploaded = Image.open(d)

print(uploaded)


# 변수에 로드된 이미지 파일을 원하는 폴더로 이동
filename = os.path.basename(file_path)
dst_path = os.path.join(upload_folder, filename)
print(f'move {filename} to {dst_path}')
print('이게 dst = ' + dst_path)

#d의 위치에 있는 폴더를 q의 위치에 있는 폴더로 이동
shutil.move(d, q)

# Commented out IPython magic to ensure Python compatibility.
#파일을 이동시키고나서는 Real-ESRGAN을 돌려야하기에 해당 디렉토리로 이동
!pwd
# %cd Real-ESRGAN

# if it is out of memory, try to use the `--tile` option
# We upsample the image with the scale factor X3.5
# 파서 잘 붙여서 추론 돌리기 (밑은 파서 내용))
!python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 3.5 --face_enhance
# Arguments
# -n, --model_name: Model names
# -i, --input: input folder or image
# --outscale: Output scale, can be arbitrary scale factore.

# api에서는 필요 없지만 테스트니까 넣은 이미지 보여주는 코드
# utils for visualization
import cv2
import matplotlib.pyplot as plt
def display(img1, img2):
  fig = plt.figure(figsize=(25, 10))
  ax1 = fig.add_subplot(1, 2, 1)
  plt.title('Input image', fontsize=16)
  ax1.axis('off')
  ax2 = fig.add_subplot(1, 2, 2)
  plt.title('Real-ESRGAN output', fontsize=16)
  ax2.axis('off')
  ax1.imshow(img1)
  ax2.imshow(img2)
def imread(img_path):
  img = cv2.imread(img_path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  return img

# display each image in the upload folder
import os
import glob

input_folder = 'upload'
result_folder = 'results'
input_list = sorted(glob.glob(os.path.join(input_folder, '*')))
output_list = sorted(glob.glob(os.path.join(result_folder, '*')))
for input_path, output_path in zip(input_list, output_list):
  img_input = imread(input_path)
  img_output = imread(output_path)
  display(img_input, img_output)

